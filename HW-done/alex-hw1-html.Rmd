---
title: "alex-hw1-html.rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bnlearn)
```

## Question 1: Building a DAG (5 points)


### 1.1
Write out the factorization of the joint distribution implied by the DAG using mathematical notation. (1 point)

$P(A, E, S, O, R, T) == P(A)P(S)P(E|A,S)P(O|E)P(R|E)P(T|O,R)$

### 1.2 
Rewrite the above factorization in *bnlearn*'s string representation. (1 point)
```{r dagStr}
dagStr <- "[A][S][E|A:S][O|E][R|E][T|O:R]"
```


### 1.3
Use this to create a DAG in *bnlearn*. (1 point)

```{r dag}
dag <- model2network(dagStr)
```

### 1.4
Print the class of the DAG object. (1 point)
```{r class}
class(dag)
```

### 1.5
Use `graphviz.plot` to plot the DAG. (1 point)

```{r graphviz}
graphviz.plot(dag)
```


## Question 2: Experimenting with graph utilities (5 points)

### 2.1
Extract and print the nodes and arcs of the DAG you created in previous questions. (1 point)
```{r nodes}
nodes(dag)
arcs(dag)
```

### 2.2
Extract and print the parents and the children of each node using `parents` and `children` functions. (1 point)
```{r parents}
for(n in nodes(dag)) {
  cat(n, "'s parents are: '", parents(dag,n), "'.  ")
  cat(n, "'s children are: '", children(dag,n), "'")
  cat("\n")
}
```

### 2.3
Use the `mb` function to extract the Markov blanket of A, E, and T. (1 point)

```{r mb}
mb(dag, "A")
mb(dag, "E")
mb(dag, "T")
```

### 2.4
How do you identify the Markov blanket from the DAG? (1 point)
For node N Markov blanked is identified as its parents, its children and parents of those children, or
in R code:
(?) Should it be expressed in code?

### 2.5
Describe, in terms of coniditional independence (NOT in terms of the DAG) the definition of a Markov blanket. (1 point)

If M = Markov Blanket of variable Y from set of random variables S then Y when conditioned on M is indpendent on any subset X of set S provided
$X \bigcap M=\emptyset$

## Question 3: Conditional probability distribution (CPD) parameter estimation (5 points)

Bayesian network = DAG + CPD with specified parameters

### 3.1
Fit the parameters of the DAG from the data stored in survey2.txt using Bayesian estimation, and save the result into an object of class bn.fit. (2 points)
```{r bn.fit}
survey <- read.table("/Users/alex/i/causalML/HW/hw1_release/survey2.txt", header = TRUE)
survey[] <- lapply(survey, function(x) as.factor(x))
bn.bayesDefault <- bn.fit(dag, data = survey, method = "bayes")
```

### 3.2
Play with the Bayesian prior parameter **iss** and report the changes in the parameters learned from Bayesian network. Explain the changes. (3 points)
sink("bn.bayes_iss_default")
bn.fit(dag, data = survey, method = "bayes")
sink()

sink("bn.bayes_iss_1")
bn.fit(dag, data = survey, method = "bayes", iss=1)
sink()

sink("bn.bayes_iss_5")
bn.fit(dag, data = survey, method = "bayes", iss=5)
sink()

sink("bn.bayes_iss_10")
bn.fit(dag, data = survey, method = "bayes", iss=10)
sink()

sink("bn.bayes_iss_100")
bn.fit(dag, data = survey, method = "bayes", iss=100)
sink()

sink("bn.bayes_iss_1000")
bn.fit(dag, data = survey, method = "bayes", iss=1000)
sink()

Explanation of differences in conditional propabilities for various `iss` argument:

Apparently with small `iss` parameter values conditional probabilities are almost the same.
Since `iss` represent sample size, and data set has a 660 entries -
the Bayesian conditional probabily inference would have 660 "iterations" for
sample size 1, 66 iterations for sample size 10 and 1 iteration for sample size 1000.
It appears the conditional propabibilites calculated with number of iterations > 10 are
in much better agreement with values than conditional probabilies calculated with small
number of iterations (< 10). If this assumption is true than the more iterations are performed
the more precise result is. It also implies not to use large values for `iss` parameter.
